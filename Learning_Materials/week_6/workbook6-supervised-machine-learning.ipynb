{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "## Description and aims\n",
    "\n",
    "This tutorial is designed to give you your first experience of machine learning in practice by implementing a simple nearest-neighbour classifier.\n",
    "\n",
    "The learning outcomes are:\n",
    "- experience of implementing the K Nearest Neighbours classification algorithm\n",
    "- experience of using the sklearn DecisionTree classification algorithm\n",
    "-  experience of working through different preprocessing steps to try and improve the performance of your classifier\n",
    "\n",
    "and from the perspective of your programming skills\n",
    "- more experience of class inheritance \n",
    "- experience of using numpy's argmin method\n",
    "- more experience of using python dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 1: Loading and Visualising Data</h1>\n",
    "   We will start by importing and visualising the  Iris dataset used  in the lecture.\n",
    "<ul>\n",
    "    <li><b>Run the 2 code cells below</b> to load and display the iris dataset</li>\n",
    "            </ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import week6_utils as w6utils\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris flowers <img src=\"figures/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "iris_x,iris_y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "\n",
    "iris_features= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "iris_names= ['setosa','versicolor','virginica']\n",
    "w6utils.show_scatterplot_matrix(iris_x,iris_y,iris_features,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 2: Implementing K-Nearest Neighbours</h1>\n",
    "</div>\n",
    "            \n",
    "Basic process for predicting the label of a new point from the trainig set\n",
    "1. Measure distance to new point from every member of the training set\n",
    "2. Find the K Nearest Neighbours  \n",
    "   in other words, the K members of the training set with the smallest distances  (*calculated in step 1*)\n",
    "3. Count the labels that were provided for those K training items,  \n",
    "   and return the most common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours. \n",
    "\n",
    "<b>Read through the code  to get a sense for how it implements the algorithm. </b><br>\n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"figures/kNN-steps.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "\n",
    "class Simple1NNClassifier:\n",
    "    \"\"\" \n",
    "    Simple example class for 1-Nearest Neighbours algorithm.\n",
    "    Assumes numpy is imported as np and uses euclidean distance\n",
    "    \"\"\"    \n",
    "    def dist_a_b(self,a:np.array,b:np.array)->float:\n",
    "        \"\"\" euclidean distance between same-size vectors a and b\"\"\"\n",
    "        assert a.shape==b.shape, 'vectors not same size calculating distance'\n",
    "        return np.linalg.norm(a-b) \n",
    "    \n",
    "    def fit(self,x:np.ndarray,y:np.array):\n",
    "        \"\"\" just stores the data for k-nerarest neighbour\"\"\"\n",
    "        self.num_training_items = x.shape[0]\n",
    "        self.num_features = x.shape[1]\n",
    "        self.model_x = x\n",
    "        self.model_y = y\n",
    "        \n",
    "    def predict(self,new_items:np.ndarray):\n",
    "        \"\"\" makes predictions for an array of new items\"\"\"\n",
    "        num_to_predict = new_items.shape[0]\n",
    "        y_pred = np.zeros((num_to_predict),dtype=int)\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = np.zeros((num_to_predict,self.num_training_items))\n",
    "        for new_item in range(num_to_predict):\n",
    "            for stored_example in range(self.num_training_items):\n",
    "                dist[new_item][stored_example]= self.dist_a_b(new_items[new_item],\n",
    "                                                              self.model_x[stored_example ])\n",
    "\n",
    "        #make predictions: \n",
    "        closest = np.argmin(dist, axis=1) #closest has one entry for each row (item to predict)\n",
    "        for item_idx in range(num_to_predict):\n",
    "            y_pred[item_idx] = self.predict_one(item_idx, dist)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_one(self,item_idx:int,distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is just for 1 Nearest Neighbour\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx (int): item to make predciton for - i.e. idx of row in distances matrix\n",
    "        dist (numpy ndarray): array of distances between new items (rows) and training set records(columns)\n",
    "        \"\"\"\n",
    "        # we're going to use numpy's argmin method (google it)\n",
    "        # which gives us the  get indexes of column with lowest value in an array\n",
    "        idx_of_nearest_neighbour = np.argmin (distances[item_idx])\n",
    "        return self.model_y[ idx_of_nearest_neighbour]\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.1</h2>\n",
    "    <b>Run the code provided below</b> for K=1 with the two datasets and make sure you understand the outputs and how they are produced\n",
    "<ul>\n",
    "    <li>For the marks dataset this creates a plot to show a decision surface<br>\n",
    "    (you do not need to understand how the PlotDecisionSurface() methods works)</li>\n",
    "    <li>For the  iris data set this uses a confusion matrix <br> (ask the internet what a confusion matrix is if you're not sure)</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make train/test split of datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(iris_x, iris_y, test_size=0.33,stratify=iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make a model\n",
    "my_1NN_model = Simple1NNClassifier()\n",
    "\n",
    "# fit it to the training data\n",
    "my_1NN_model.fit(train_x,train_y)\n",
    "\n",
    "# use it to make predictions for test data\n",
    "predictions = my_1NN_model.predict(test_x)\n",
    "print(f' predictions are {predictions.T}') #.t turns column to row so it shows on screen better \n",
    "\n",
    "\n",
    "# make array of whether two arrays have equal values\n",
    "print ( f'individual matches to acvtual values are{test_y==predictions}')\n",
    "\n",
    "# do some counting to get the accuracy\n",
    "accuracy = 100* ( test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy} %\")\n",
    "\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(test_y[i])\n",
    "    predicted = int(predictions[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "#and here's sklearn's built-in method\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(test_y, predictions,display_labels= iris_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a 2-D version of the iris data set to illustrate the decision surface\n",
    "We will only use the two petal features so we can visualise it in 2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make data - labels are the same as before\n",
    "petals_train = train_x[:,2:4]\n",
    "petals_test = test_x[:,2:4]\n",
    "\n",
    "#make model\n",
    "my_1NN_model2 = Simple1NNClassifier()\n",
    "# fit it to data\n",
    "my_1NN_model2.fit(petals_train,train_y)\n",
    "\n",
    "#make predictions, score them \n",
    "y_pred = my_1NN_model2.predict(petals_test)\n",
    "accuracy = 100* ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")\n",
    "\n",
    "title= \"1-Nearest Neighbour on petal features\"\n",
    "w6utils.plot_decision_surface(petals_train,train_y,my_1NN_model2, title, iris_features[2:4],step_size= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.2: Create your own implementation of K-Nearest Neighbours</h2>\n",
    "    <p> Using the code above,  extend the predict method for the class Simple1NNClassifier  to use the votes from K>1 neighbours.</p>\n",
    "    <ol>\n",
    "        <li>Create a class that inherits most of the code: <code>class SimpleKNNClassifier (Simple1NNClassifier):</code> </li>\n",
    "        <li> Create a new initialisation method that takes one parameter: the number of neighbours to consider(K)<br>\n",
    "and saves it in <code>self.K</li>\n",
    "        <li> Over-ride the <code>predict_one()</code> method <br>\n",
    "        so that instead of just finding the label of the single closest neighbour it:\n",
    "         <ol> \n",
    "             <li> Finds the indexes of the <code>self.K</code> nearest neighbours.<br>\n",
    "                 HINT: you can replace <code>np.argmin</code> with <code>np.argpartition</code> <br>\n",
    "                 <a href = https://stackoverflow.com/questions/34226400/find-the-index-of-the-k-smallest-values-of-a-numpy-array>\n",
    "                 This question</a> is the same and the first answer is really useful.\n",
    "             </li>\n",
    "             <li>Stores the labels of these instances.<br>\n",
    "                 The most general way to do with without making assumptions is to use a dictionary<br>\n",
    "             but this will mean explicitly casting labels to strings to be safe</li>\n",
    "             <li> Iterates through the labels to see which is most popular. <br>\n",
    "             You may find the reminder below if you are not used to python dictionaries</li>\n",
    "             <li> returns the most popular label as the prediction for item</li>\n",
    "         </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:lightblue;color:black\">\n",
    "    <h3> Reminder: Storing data in python dictionaries and iterating through their contents</h3>\n",
    "    <p> Python dictionaries are a way of storing data that can be accessed via a key<br>\n",
    "for example: <code> my_dict= {'name':'jim','familyname':\"Smith\", 'job':'professor'}</code><br>\n",
    "<b>Keys are usually strings</b>, but the values associated with a key can be any type, including numbers.</p>\n",
    "\n",
    "<p> The following snippets of code might be useful to you - <b>after</b> you have edited them.</p>\n",
    "<p> Make a new code cell in the notebook, copy the snippets in and run it, then edit it as you need.</p>\n",
    "<p><pre style='background:lightblue;colour:black'>    \n",
    "labels = ['a','b','a','c','a','d','b']\n",
    "indexes = [1,4,6]\n",
    "mydict={}\n",
    "<span style=\"color:green\">for</span> idx <span style=\"color:green\">in</span> indexes:\n",
    "    <span style=\"color:green\">if</span> labels[idx] <span style=\"color:green\">in</span> mydict.keys():\n",
    "        mydict[labels[idx]] += 1\n",
    "    <span style=\"color:green\">else</span>: #create a new dictionary entry if needed\n",
    "        mydict[labels[idx]] = 1\n",
    "<span style=\"color:green\">print</span>(f'mydict is {mydict}')\n",
    "\n",
    "leastvotes=99\n",
    "<span style=\"color:green\">for</span> key,val <span style=\"color:green\">in</span> mydict.items():\n",
    "    <span style=\"color:green\">if</span> val < leastvotes:\n",
    "        unpopular= key\n",
    "        leastvotes=val\n",
    "<span style=\"color:green\">print</span>(f'{unpopular}, {leastvotes}')\n",
    "    </pre></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleKNNClassifier(Simple1NNClassifier):\n",
    "    \"\"\"\n",
    "    Complete this class to prodiuce a KNN classifier\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" your code here\n",
    "        you will need to change the function signature\n",
    "        \"\"\"\n",
    "        \n",
    "    def predict_one(self,item_idx:int,distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        You should write this to accept any number of neighbour K\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx (int): item to make predciton for - i.e. idx of row in distances matrix\n",
    "        dist (numpy ndarray): array of distances between new items (rows) and training set records(columns)\n",
    "        \"\"\"\n",
    "        prediction = -99999 #dummy value\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 2.3: Test your implementation on the iris dataset</h2>\n",
    "<p>Use the toolbar to copy and paste the second and third  cells from activity 2.1 below here. <br>\n",
    "Then edit them so that they create and use objects of your new class, instead of the class Simple1NNClassifier.\n",
    "\n",
    "Start with K=1 - this should produce   the same results as you got with my code in activity 2.1, then try with K = {3,5,7}\n",
    "<ul>\n",
    "    <li>Make  <b>qualititative</b> judgements : how does the decision surface change?</li>\n",
    "    <li>Make <b>quantitative</b> judgements :  how does the confusion matrix change?</li>\n",
    "    <li> In Machine Learning we talk about algorithms having  <b>hyper-parameters</b> that control their behaviour.<br>\n",
    "        Adapt your code to investigate:\n",
    "        <ul>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>test</b> set?</li>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>test</b> set?</li>\n",
    "            <li> If these are not the same, can you explain why not?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h1> Activity 3: Decision Trees</h1></div>\n",
    "\n",
    "In the lecture notebook we illustrated how the decision tree is created by a process of expanding nodes.\n",
    "\n",
    "We often want to control how we learn a model (in this case, grow a tree) h to avoid a phenomenon call **over-fitting**.\n",
    "\n",
    "- This is where the model is capturing fine-details of the training set and so failing to generalise from the training set to the real world.\n",
    "- like in the images where all the dogs faced left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 3.1: exploring how to control tree-growth to prevent over-fitting</h2>\n",
    "The aim of this activity is for you to experiment with what happens when you change three <b>hyper-parameters</b> that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li> max_depth: default is None)</li>\n",
    "    <li>min_samples_split: default value is 2</li>\n",
    "    <li>min_samples_leaf: default value is 1</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "Experiment with the Iris data set we loaded earlier to see if you can work out what each of these hyper-parameters does, and how it affects the tree. \n",
    "<ul>\n",
    "<li> If you uncomment the first line after the imports, it will give you a different train-test split of the Iris data each time you run it.<br>\n",
    "    Does this affect what tree you get? </li>\n",
    "    <li> Is there a combination of hyper-parameter values that means you consistently get similar trees?</li>\n",
    "    <li>    What is a good way of judging 'similarity?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# do a new split of the data into into train:test\n",
    "#train_x, test_x, train_y, test_y = train_test_split(iris_x, iris_y, test_size=0.33,stratify=iris_y)\n",
    "\n",
    "\n",
    "\n",
    "## Experiment with changing these values\n",
    "depth= 1 #  try 1,3,5\n",
    "minsplit = 2 #try 2,5\n",
    "minleaf=1 #try 1,5\n",
    "\n",
    "#make a model with those hyper-parameters\n",
    "model = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=minsplit,min_samples_leaf=minleaf)\n",
    "model.fit(train_x,train_y)\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "accuracy = 100* ( test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")\n",
    "\n",
    "\n",
    "CMPlot=ConfusionMatrixDisplay.from_predictions(test_y,predictions, display_labels=iris_names)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10)) #you may need to increase the figure size for larger trees\n",
    "_ = tree.plot_tree(model, feature_names=iris_features,  class_names=iris_names, filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" > <h1> Activity 4: Creating a test harness for comparing ML algorithms on a dataset</h1>\n",
    "<p> Now you have done some manual experimenting with different hyper-parameter values for algorithms, it's time to think about automating that process.</p>\n",
    "<p>Complete the cell below to create a method that: </p>\n",
    "<ul>\n",
    "    <li> Takes a train and test data  arrays as  parameters <br>\n",
    "        HINT: develop your code using train_x, test_x,train_y,test_y for the iris data from above</li>\n",
    "    <li> Runs your SimpleKNNClassifier with K={1,3,5,7,9} and stores the test accuracy for each <br>\n",
    "    HINT: you could use:\n",
    "        <ul>\n",
    "            <li>a for loop to run the algorithm with different settings k  for  the number of neighbours(K),</li>\n",
    "            <li> an <a href=https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/>f-string</a> e.g. <code>experiment_name= f'KNN_K={k}'</code> to create a meaningful name for each run </li>\n",
    "            <li>a   dictionary to store your results, where each experiment has the string <em>experiment_name</em> as the key and the accuracy as the value </li>\n",
    "            </ul>for for this?</li>\n",
    "    <li> Runs a DecisionTreeClassifier with all the different combinations of hyper-parameters from activity 3<br>\n",
    "       HINT: You could do this in the same way as I've suggested above but with nested for-loops (one for each hyper-parameter) and a more complex python f-string to create the name (key), then store the results in the same dictionary.  </li>\n",
    "    <li> Reports the results and which algorithm-hyperparameter combination has the highest test accuracy</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_ml_test_harness(train_x:np.ndarray,train_y:np.ndarray,\n",
    "                          test_x:np.ndarray,test_y:np.ndarray):\n",
    "    \"\"\" code to compare supervised machine learning algorithms on a dataset\"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    print('not implemented yet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now run your code for the iris data\n",
    "first_ml_test_harness(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
