{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4600f86a-b35a-4dfb-8757-3f6f6031c1ed",
   "metadata": {},
   "source": [
    "# Portfolio part 4 (workbook 8) Self-Checker\n",
    "\n",
    "This notebook is designed to stream line the process of checking and improving \n",
    "the two assessed activities from workbook 8.\n",
    "\n",
    "It is specifically designed to:\n",
    "- reduce frustration that happens when the marking system rejects or will not run your code.\n",
    "- maximise your opportunities for getting useful feedback  \n",
    "\n",
    "We **strongly recommend** that you use this to test your code prior to submission rather than waste any attempts on code that would fail to run on the marking server.\n",
    "\n",
    "## How to use:\n",
    "- work through this notebook making sure you run all the cells\n",
    "- *especially* the one which create a set of 'allowed imports' so you can do a single import  \n",
    "   (this reomvoes a lot of the headaches around people get submissios rejected\n",
    "- copy and paste your code into the cells indicated.\n",
    "- when you run these cells irt wil lwritew your code into a file 'student.py'\n",
    "- afterwards cells will import your code back into the notebook and run the same code that is present on the marking server.\n",
    "\n",
    "- **Please note:  Although the code is the same, the datasets used to test your workflow may be different on the marking server**\n",
    "\n",
    "### When you are happy with your work, we recommend that you\n",
    "1. Select  'kernel-> restart kernel and clear all outputs' from the top menu\n",
    "2. Run all the cells in order, making sure all the outputs are ok.\n",
    "3. Download the file student.py ready for submission.\n",
    "\n",
    "\n",
    "### The next cell creates a set of standard imports that provide all the functionality you need, and writes them to file so you can do a single import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821c32b-da91-418f-9220-6e3ada36391f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile \"approvedimports.py\"\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa89e46-8409-40e4-a33e-4cf302f15715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from approvedimports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f0f6e-f140-41fc-9673-6d9a8a799530",
   "metadata": {},
   "source": [
    "## Testing activity 1.1: Evaluating Reliability and efficiency as network size grows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007a0eb-a00e-46cb-90e7-05d0730b68a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    " <h2> Activity 1.2 (Assessed) <br>Automating the investigation of the effect of model <i>capacity</i> on learning behaviour</h2>\n",
    "    <h3> 20 Marks:</h3>\n",
    "    <ul>\n",
    "        <li>0 marks if the code cell with the function <code>make_reliability_plot()</code> contains any text outside the function body</li>\n",
    "        <li> 0 marks if your code does not return the fig and axes objects as required</li> \n",
    "    <li>10 marks for producing a matplotlib figure containing two matplotlib ax objects with titles and labels as specified below,<br>\n",
    "    and returning the objects (i.e. a figure and an array of axes) </li>\n",
    "    <li> 5 marks each if the contents of the plots match the <i>reference version</i>.<br> This means you <b>must</b> set the <i>random_state</i> hyperparameter for each run as described below</li>\n",
    "    </ul>  \n",
    "<p></p>\n",
    "\n",
    "<h3>Task definition:</h3> Complete the function in  the  cell below to <i>automate</i> the process of investigating the effect of the model <i>capacity</i> (as controlled by <i>hidden_layer_sizes</i> hyper-parameter) for a MLP with a single layer of hidden nodes on:\n",
    "<ul> <li>the <i>reliability</i> - as measured by the <i> success rate</i> i.e. the proportion of runs that achieve 100% training accuracy</li>\n",
    "<li>  the <i> efficiency</i> - the mean number of training epochs per successful run.<br>\n",
    "    Note that to avoid <i>divide-by-zero</i> problems you should check if no runs are successful for a given value and report a value of 1000 in that case.  </li>\n",
    "    </ul>\n",
    "<p>What should be in the plots?</p>\n",
    "<ul>\n",
    "    <li> You must return two objects <i>fig</i> and <i>axs</i> produced by a call to <code>plt.subplots(1,2)</code></li>\n",
    "    <li> The left hand plot should have a title \"Reliability\", y-axis label \"Success Rate\" and x-axis label \"Hidden Layer Width\".</li>\n",
    "    <li> The right hand plot should have a title \"Efficiency\", y-axis label \"Mean epochs\" and x-axis label \"Hidden Layer Width\".</li>\n",
    "    <li> In both cases the width of the single hidden layer should cover the range 1,10 (inclusive) in steps of 1</li>\n",
    "    <li> Each plot should contain an appropriate line illustrating the results of the experiment</li> \n",
    "</ul>    \n",
    "<h3>How to go about the task</h3> \n",
    "    <p> In several of the stages below you will be adapting code from activity 1.1 and 'steps' refer to comments  and code snippets in that code cell.</p>\n",
    "<ol>\n",
    "    <li> Declare a list <code>hidden_layer_width</code> holding the values 1 to 10 (inclusive) defining the model size.</li>\n",
    "    <li> Declare a 1-d numpy array filled with zeros  called <code>successes</code> to hold the number of successful runs for the different model sizes.</li>\n",
    "    <li> Declare a 2-D numpy array filled with zeros of shape (10,10) called <code>epochs</code> \n",
    "    <li> Create two nested loops: one over all the values for a variable <code>h_nodes</code> from the list <code>hidden_layer_width</code> <br> and the other for a variable <code>repetition</code> between 0 and 9 (i.e. doing 10 repetitions).</li>\n",
    "    <li> Inside those loops \n",
    "        <ol>\n",
    "        <li>Copy and edit code from  step 3 from the first cell to create an MLP with one hidden layer containing the <i>h_nodes</i> nodes. <br><b>Make sure</b> that in the call  you set the parameter <i>random_state</i> to be the run index so the results are the same as mine.  </li>\n",
    "        <li>Copy and edit code from step 4 to  <i>fit</i> the model to the training data, </li>\n",
    "        <li>Copy and edit code from Step 5 to measure it's accuracy</li>\n",
    "            <li> If the accuracy is 100%:<ul>\n",
    "                <li><i>increment</i> the count  in  cell  <code>successes[hnodes]</code></li>\n",
    "            <li> store the number of epochs taken in the cell of the array <code>epochs[h_nodes][repetition]</code>.</li>\n",
    "            </ul>\n",
    "        </ol>\n",
    "    <li> Create a new array with one entry for each number of hidden nodes tested, that contains either:\n",
    "        <ul>\n",
    "            <li> 1000 if no runs got 100% accuracy for that network size</li>\n",
    "            <li> The mean number of epochs taken per successful run for that network size</li>\n",
    "        </ul>\n",
    "    <li>Copy and edit the code from step 6 in Activity 1.1 to make a figure contain two plots side-by-side as described in the task definition, set appropriate axis labels and title labels, and return the fig and axs objects </li>\n",
    "</ol>\n",
    "    <h3> Checklist before submission</h3>\n",
    "    <ul>\n",
    "    <li> The second cell below will let you test your code works before submission. </li>\n",
    "        <li> The marking server will reject your submission if there is any text or code  in the second cell that it outside inside the function definition.</li>\n",
    "        <li> Your function <b>must</b> return two things: the fig object, and the axs object (which should be an array of axes with shape (1,2).</li>\n",
    "     </ul>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ee2d0-4301-485e-8757-5e171a90874e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create XOR data\n",
    "xor_x= np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13213c05-bd63-44d0-be9c-1fd98054eb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile \"student.py\"\n",
    "# you must have the following line and no others which have the word imp0rt (with o replacing 0)\n",
    "#  it's not even allowed in this comment!\n",
    "\n",
    "from approvedimports import *\n",
    "\n",
    "# replace the empy definition below with your code\n",
    "def make_xor_reliability_plot(train_x, train_y):\n",
    "    \"\"\" Insert code below to  complete this cell according to the instructions in the activity descriptor.\n",
    "    Finally it should return the fig and axs objects of the plots created.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_x: numpy 2Dndarray R rows x f features\n",
    "    train_y: numpy 1Darray R rows\n",
    "    \"\"\"\n",
    "    fig = \"change this line\"\n",
    "    axs= \"change this line\"    \n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c71f2-9151-4f55-bd11-b2f5a87a997b",
   "metadata": {},
   "source": [
    "### Next cell calls code that duplicates what is on the marking server\n",
    "Run it to see what mark you should get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740c2a6-9176-4ac0-ab9b-6d0c31760e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from approvedimports import *\n",
    "#reload the student's work if that cell has been edited and run in the notebook\n",
    "import student \n",
    "reload (student)\n",
    "from student import make_xor_reliability_plot\n",
    "\n",
    "\n",
    "from test import test_make_xor_reliability_plot\n",
    "#get rid of pre-existing variables to make sure behaviour mimics marking server\n",
    "if 'myfig' in globals():\n",
    "    del(myfig)\n",
    "if 'myaxs' in globals():\n",
    "    del(myaxs)\n",
    "\n",
    "\n",
    "try:\n",
    "    myfig,myaxs = make_xor_reliability_plot(xor_x,xor_y)\n",
    "    score, feedback= test_make_xor_reliability_plot(myfig,myaxs)\n",
    "    print(f'Score {score}\\n{feedback}')\n",
    "\n",
    "except Exception as e:\n",
    "    print ( \"method did not return two objects (fig and axs) as required.\\n\"\n",
    "               f' {e}\\n'\n",
    "               \"Fix this before trying to get a mark\"\n",
    "              )\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488b65b-c8d0-4905-b8f8-8692c1f43364",
   "metadata": {},
   "source": [
    "## Testing activity 2.3:\n",
    "### Creating a test workflow to fairly assess three different supervised learning algorithms on a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36519389-c819-4e96-ad90-0abfc0ec8798",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-warning\" style=\"color:black\">\n",
    "    <h2>Activity 2.3 Assessed: <br>\n",
    "    Creating a test workflow to fairly assess three different supervised learning algorithms on a dataset</h2>\n",
    "    <h3> 80 marks</h3>\n",
    "    <h4> Task Description: </h4>\n",
    "    <p> Complete the functions in the skeleton class (obeying any instructions in the method docstrings about types and names of variables) below to create a class with the following functionality listed below:\n",
    "        <ol>\n",
    "            <li> The <code>__init__</code> method should read in and store a set of input examples and labels<br>\n",
    "            from two files whose names are provided at run-time <b>(10 marks)</b></li>\n",
    "            <li> The <code>preprocess()</code> method should perform any preprocessing of the stored input examples needed to ensure the comparison between algorithms is fair.<b>(10 marks)</b></li>\n",
    "            <li> The <code>make_label_encoders</code> method should check whether there are more than two labels present in <i>data_y</i>,<br>\n",
    "    and if so make any different encodings of the labels needed for different classifiers.<b>(10 marks)</b></li>\n",
    "            <li> The <code>run_comparison()</code> method should do a fair comparison of the classifier versions of k-Nearest Neighbour, DecisionTree and MultilayerPerceptron algorithms, and store the best accuracy for each.<br>\n",
    "            <i>Fair</i> means doing hyper-parameter tuning for the combinations of values given below and storing each trained model.<b>(3 x 10 marks)</b><br>\n",
    "            Models should be saved by appending to a list held as the value in a dictionary <code>self.stored_model</code>(see below for details).<br>You are encouraged to use the scikit-learn versions of all three algorithms as they have common interfaces which will make your coding easier.</li>\n",
    "            <li> The best comparison result for each algorithm, and the location of the stored model, should be stored by creating and then adapting dictionaries called <br>\n",
    "            <code>self.best_model_index:dict = {\"kNN\":0, \"DecisionTree\":0 and \"MLP\":0}</code> and <br>\n",
    "             <code>self.best_accuracy:dict = {\"kNN\":0, \"DecisionTree\":0 and \"MLP\":0}</code> <b>(10 marks)</b>\n",
    "</li>\n",
    "    <li> The <code>report_best()</code> method should report the best performing model, in the format specified.<b>(10 marks)</b></li>\n",
    "    </ol>\n",
    "    <p> For the KNearestNeighbor algorithm you should try K values from the set {1,3,5,7,9}</p>\n",
    "    <p> For DecisionTreeClassifer you should try every combination of <br>\n",
    "    <i>max_depth</i> from the set {1,3,5} with<br>\n",
    "    <i>min_split</i> from the set {2,5,10} and <br>\n",
    "    <i> min_samples_leaf</i> from the set {1,5,10}.</p>\n",
    "    <p> For MultiLayerPerceptron you should try every combination of <br>\n",
    "    <i>first hidden layer width</i> from the set {2,5,10} with<br>\n",
    "    <i>second hidden layer width</i> from the set {0,2,5} and<br>\n",
    "    <i> activation</i> from the set {\"logistic\",\"relu\"}.</p>\n",
    "    <h4> How to begin?</h4>\n",
    "    <p>This task builds heavily on  the code in this notebook, and that you wrote in worksheet 6 activity 4.\n",
    "    So make sure you have completed that activity before attempting this task.</p>\n",
    "    <h4> Things you must do so we can mark your code and provide feedback  automatically</h4>\n",
    "    <ul> \n",
    "    <li> The examples and labels should be stored in arrays <code>data_x</code> and <code>data_y</code> </li>\n",
    "    <li> The constructor should  create a dictionary to hold all the stored models<br> <code> self.stored_models:dict={\"KNN\":[],\"DecisionTree\":[],\"MLP\":[]}</code> </li>\n",
    "    <li> As your code creates and fits models of different types they should be appended to the relevant list in the <i>stored_models</i> dictionary.<br>\n",
    "        i.e., each different MLP model gets appended to the list <i>self.stored_models[\"MLP\"]</i> after the call to <i>fit()</i></li> \n",
    "    <li>It probably makes sense to check and update the values held in <i>self.best_accuracy</i> and <i>self.best_model_index</i> as you test each model</li>\n",
    "    <li> It is acceptable to do only one run of each algorithm-hyperparameter combination</li>\n",
    "    <li> Any code that takes a <i>random_state</i> parameter should be given the value 12345</li>\n",
    "    </ul>\n",
    "    <div style=\"background:lightgreen\"><h2> Don't over-think this!</h2><ul> \n",
    "        <li>You have most of the code snippets you need,</li>\n",
    "        <li>and the hyper-parameter tuning is mostly a case of nested loops to run through combinations of values</li>\n",
    "        <li> and from the search topic you should be used to keeping track of 'best-so-far' as you go through options</li></ul></div>\n",
    "    <h4>Remember the marking system will not accept code cells if you have anything outside your class definition</h4>\n",
    "    <h4> The point is that your code should work for different datasets - so don't hard code things about the data</h4> \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb92db-5d29-450e-9c67-8fd9479614a6",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "1. This page [sklearn user guide on scaling](https://scikit-learn.org/stable/modules/preprocessing.html#) gives a good overview on how to scale data.\n",
    "- I recommend you use a MinmaxScaler \n",
    "- Remember that the idea of splitting data into train and test is to simulate what will happen once the model is deployed and encounters data it has never seen before.\n",
    "- That means you must fit the scaler to the training data (not all the data) i.e. do you train-test-split first\n",
    "\n",
    "2. If there a re more than two unqie labels present you will need to create  a onehot encoding of them to use with the MLP.\n",
    "- sklearn provides a LabelBinarizer class to do this [Description of how to use labelbinarizers](https://scikit-learn.org/stable/modules/preprocessing_targets.html#labelbinarizer) \n",
    "- Doing it using this vclass is *safest* because it makes the fewest assumptions about the labels (i.e. it can cope with labels that are [0,2,5] as well as  [0,1,2]) \n",
    "\n",
    "3. If you want to be really *pythonic* you can use the zip function for the hyper-parameter tuning,  \n",
    "   but for simplicity, for all three classifiers its easiest to make a list of values for each of the parameters you are asked to tune and then  use nested loops to iterate over them  \n",
    "- so if algorithm X has two params A and B you could make  lists ``` a_values =  [a1,a2,a3], b_values= [b1,b2]```  \n",
    "  and then do  \n",
    "```` \n",
    "  for aval in a_values:\n",
    "      for bval in b_values:\n",
    "         nextclassifier = X(paramA=aval,paramB=bval)\n",
    "         ....\n",
    "````\n",
    "    \n",
    "\n",
    "4. To make life easier, in my version of the MLP I created a set of tuples holding the hiden layer sizes to iterate over\n",
    "```layers= [(2,),(5,),(10,),(2,2),(5,2),(10,2),(2,5),(5,5),(10,5)] ```\n",
    "\n",
    "5. All of these sklearn version of classifiers support:\n",
    "- a *fit()* method (that your code should call with parameters  ```self.train_x``` and ```train_y``` and \n",
    "- a *score()* method, that returns a float (accuracy)  that your code should call with parameters ```self.test_x``` and ```test_y```   \n",
    "  where ```train_y, test_y``` are the *raw* or one_hot encoded versions of the labels depending on the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e271e-bfde-468b-93cb-e8bb6d456361",
   "metadata": {},
   "source": [
    "### How to get started\n",
    "\n",
    "The next cell starts with a %%writefile command to append the rest of the cel contents to student.py so you can submit it when you are ready\n",
    "\n",
    "As above, you must include the line which does the imp0rts without changing it (notice how careful I'm being with my spelling here - even comments matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80c18b-2b14-4246-8592-cabd534781b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a \"student.py\"\n",
    "\n",
    "from approvedimports import *\n",
    "\n",
    "# complete the code skeleton below\n",
    "class MLComparisonWorkflow:\n",
    "    \"\"\" class to implement a basic comparison of supervised learning algorithms on a dataset \"\"\" \n",
    "    \n",
    "    def __init__(self,datafilename:str,labelfilename:str):\n",
    "        \"\"\" Method to load the feature data and labels from files with given names,\n",
    "        and store them  in arrays called data_x and data_y.\n",
    "        \n",
    "        You may assume that the features in the input examples are all continuous variables\n",
    "        and that the labels are categorical, encoded by integers.\n",
    "        The two files should have the same number of rows.\n",
    "        Each row corresponding to the feature values and label\n",
    "        for a specific training item.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\" Method to \n",
    "           - apply the preprocessing you think suitable to the data\n",
    "           - separate it into train and test splits (using a 70:30 division)\n",
    "           Remember to set random_state = 12345 if you ue train_test_split()\n",
    "        \"\"\"\n",
    "        self.stored_models:dict={\"KNN\":[],\"DecisionTree\":[],\"MLP\":[]}\n",
    "                                 \n",
    "                                 \n",
    "                                 \n",
    "    def make_label_encodings(self):\n",
    "        \"\"\" Method to make one-hot encodings if the data has more than two labels.\n",
    "        Note you will probably need to keep the original label array for some algorithms\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def run_comparison(self):\n",
    "        \"\"\" Method to perform a fair comparison of three supervised machoinbe learning algorithms.\n",
    "        Should be extendable to include more algorithms later.\n",
    "        \n",
    "        For each of the algorithms KNearest Neighbour, DecisionTreeClassifer and MultiLayerPerceptron\n",
    "        - Applies hyper-parameter tuning to find the best combination of relvant values for the algorithm\n",
    "         -- creating and fitting model for each combination, \n",
    "            then storing it in the relevant list in a dictionary called self.stored_models\n",
    "            which has the algorithm names as the keys and  lists of stored models as the values\n",
    "         -- measuring the accuracy of each model on the test set\n",
    "         -- keeping track of the best performing model for each algorithm, and its index in the relevant listso it can be retrieved.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def report_best(self) :\n",
    "        \"\"\"\n",
    "        Method to analyse results.\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy (float) - the accurcy of the best performing model\n",
    "        algorithm (str) - one of \"KNN\",\"DecisionTree\" or \"MLP\"\n",
    "        model (fitted model of relvant type)- the actual fitted model to be interrogated by marking code.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf68437-b888-40e0-bb68-30c26717718e",
   "metadata": {},
   "source": [
    "### Run the next two cells to test your code prior to submission\n",
    "- the first cell defines the test function\n",
    "- the second one calls this test function using the iris data to test your code\n",
    "- **Note** on the marking server I may use a different dataset\n",
    "- so your code should not assume anything about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce38fb-1a4e-4a82-8f6e-b8211d154a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from test import test_mlcomparisonworkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a56ec7-6da8-494c-9c93-19f4fc1269f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use iris data for this pre-submission test\n",
    "from sklearn.datasets import load_iris\n",
    "iris_x, iris_y = load_iris(return_X_y=True)\n",
    "\n",
    "#reload the student's work if that cell has been edited and run in the notebook\n",
    "reload(student)\n",
    "from student import MLComparisonWorkflow as student_version\n",
    "\n",
    "#call the test method, passing it the student;s implemtastion of the workflow class\n",
    "score,feedback =test_mlcomparisonworkflow(student_version,iris_x,iris_y)\n",
    "print(f'at this stage your code scores {score}, with feedback:\\n'\n",
    "     f'{feedback}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55618d-5595-4d85-8db0-0adae468ce3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
