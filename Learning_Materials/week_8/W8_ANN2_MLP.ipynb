{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#    Multi Layer Perceptrons\n",
    "## Artificial Intelligence 1, week 8\n",
    "\n",
    "This week:\n",
    "- recap on perceptrons\n",
    "- perceptrons are linear classifiers\n",
    "- multi-layer perceptrons\n",
    "   - architecture\n",
    "   - feed forward predictions\n",
    "   - back propagation for training\n",
    "   - examples\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neurons - The basis of  Neural Networks\n",
    "\n",
    "<div>\n",
    "<div style=\"float:left\">\n",
    "<p>Perceptrons, invented by Frank Rosenblatt in the late 1950's,<br>\n",
    "    are a form of supervised machine learning algorithm inspired by neuron cells.<br><br>\n",
    "    In neurones:</p>\n",
    "    <ul>\n",
    "    <li>signals come in along the dendrites and out along the axon.</li> \n",
    "    <li>Synapses connects a cell's dendrites to other cells' dendrites.</li>\n",
    "<li>Crudely, input signals are 'summed' <br>\n",
    "    and if they reach a certain threshold the neuron 'fires'<br>\n",
    "and sends a signal down the synapse to the\n",
    "    connected cells.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div style=\"float:right\"><img src=\"figures/neuron.jpg\" width=400></div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons - The basis of Artificial Neural Networks\n",
    "<div>\n",
    "    <div style=\"float:left\" width=400>\n",
    "<p>Perceptrons are an algorithmic approximation of this process<br> and can learn to solve simple classification problems.</p>\n",
    "<ul>\n",
    "    <li>Input values are multiplied by a learnable parameter called a <b>weight</b>.</li>  \n",
    "    <li>If the sum of the inputs $\\times$ weights is over a certain threshold (0), <br>\n",
    "    the Perceptron 'fires' and generates an output.</li>\n",
    "    <li> To let us adapt the threshold in the same way as the weights we provide a <b>bias</b> signal.</li>\n",
    "    <li>During training we iterate through a process of:\n",
    "        <ol> \n",
    "            <li>Present a training item and calculate the perceptrons output</li>\n",
    "            <li> Calculate the  error for that item </li>\n",
    "           <li>Use the <b>error</b> to change the weights by a small amount (the learning rate).</li>\n",
    "       </ol>\n",
    "    </li>\n",
    "    <li>The process is repeated until the error is 0, or as small as we can get it.</li>\n",
    "</ul>\n",
    "    </div>\n",
    "<div style=\"float:right\"><img src=\"figures/Perceptron.png\", width=400/></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrons Create Linear Decision Boundaries\n",
    "\n",
    "<img src=\"figures/straightLine.png\" style=\"float:right\" width=300>\n",
    "\n",
    "\n",
    "### Perceptron's output changes when the sum of the weighted  inputs is 0  \n",
    "So if we plot the output at different points in space there is a  *decision boundary*.\n",
    "\n",
    "This happens when  $input1 \\times weight1 + input2 \\times weight2 + biasweight = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can rearrange this equation to see that the change of behaviour happens when   \n",
    "$input2 = - \\frac{weight1}{ weight2} \\times input1  - \\frac{ biasweight}{ weight2}$\n",
    "\n",
    "But this is just the equation for a straight line !\n",
    "- slope (m) is given by -( weight1/ weight2)\n",
    "- the intercept, c  = -( biasweight / weight2)\n",
    "\n",
    "- step function => which side of the line is 1 or 0\n",
    "\n",
    "So, the Perceptron is essentially learning a function for a straight line which is called the decision boundary.\n",
    "In this case, which 'class' the set of inputs belongs to i.e. True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "import W8_utils as utils\n",
    "%matplotlib inline\n",
    "\n",
    "weight1 = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "weight2 = widgets.FloatSlider(value=0.5,min = -1,max = 1)\n",
    "biasweight = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "funcToModel = widgets.RadioButtons(options=['OR','AND','XOR'])\n",
    "output=interact(utils.showPerceptron, w1=weight1,w2=weight2,bias=biasweight,func = funcToModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Training Law\n",
    "Every time you present an example you compare the output to the desired value,  \n",
    "then update each weight in turn using:\n",
    "\n",
    "    ∆ω_i = ε · input_i · α\n",
    "\n",
    "    change in weight_i  = error   X input_i  X learning rate (fixed)\n",
    "\n",
    "So this means that \n",
    "- Error = target-actual, can be negative \n",
    "- Weights only change when there is an error \n",
    "- Only active inputs are changed. \n",
    "- Inactive (x=0) inputs are not changed at all (which makes sense since they did not contribute to the error). \n",
    "\n",
    "See AI illuminated p297+ for a worked example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of how you might code a perceptron classifier with sklearn-like methods\n",
    "1 Update the simple perceptron we created in week 7\n",
    "- to have methods for *fit()* *predict()* nd *predict_proba()* (output probabilites per class)\n",
    "- to be configurable to any number of inputs\n",
    "\n",
    "2 Then we will use it to train a model (*fit* it to the data)\n",
    "- to predict  \"setosa- not setosa\" for the iris data, \n",
    "- using the first two features for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Jim's example code for a simple perceptron\"\"\"\n",
    "    \n",
    "    def predict(self, input_records:np.ndarray) -> int:\n",
    "        \"\"\"makes predictions for a set of input examples\"\"\"\n",
    "        ypred= np.zeros(input_records.shape[0])\n",
    "        for row in range(input_records.shape[0]):\n",
    "            #calculate sum of weights * inputs\n",
    "            summed_input =  1*self.bias_weight         #bias is always +1\n",
    "            for feature in range (self.n_inputs):    \n",
    "                summed_input += self.weights[feature] * input_records[row,feature]\n",
    "            #perceptron applies simple thresholding to get output\n",
    "            if summed_input > 0:\n",
    "                ypred[row]= 1\n",
    "        return ypred\n",
    "\n",
    "    def fit(self,train_X,train_y, max_epochs=10, learning_rate=0.1):\n",
    "        \"\"\" fits the percepton to the data\"\"\"\n",
    "        #create right number of random input weights\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_inputs= train_X.shape[1]\n",
    "        self.weights= np.random.rand(self.n_inputs)\n",
    "        self.bias_weight=np.random.rand()\n",
    "        errors,epoch =99999,0\n",
    "        while epoch < max_epochs  and errors:\n",
    "            self.update_weights(train_X,train_y)\n",
    "            errors= (train_y != self.predict(train_X)).sum()\n",
    "            epoch += 1\n",
    "        print(f\" After epoch {epoch} there were {errors} errors\")\n",
    "\n",
    "            \n",
    "    def update_weights( self, input_data:np.ndarray, targets:np.array):\n",
    "        \"\"\" does weight updates for each example in turn\"\"\"\n",
    "        for row in range(input_data.shape[0]):\n",
    "            row_as_nparray= input_data[row].reshape(1,input_data.shape[1])\n",
    "            prediction= self.predict(row_as_nparray)\n",
    "            error= targets[row]- prediction[0]\n",
    "            if error != 0:\n",
    "                self.bias_weight += error * 1   * self.learning_rate \n",
    "                for feature in range (self.n_inputs):\n",
    "                    self.weights[feature]    += error * input_data[row,feature] * self.learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptrons - Behaviour on Real Data \n",
    "### We'll train two classifiers from the iris data: one each to recognise setosa and virginica varieties\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.  \n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In supervised learning, especially with neural networks, we generally:\n",
    "1.  **Preprocess** the data so each feature is scaled to the range 0,... 1, \n",
    "   - so that each feature is of equal importance\n",
    "2. Identify **Stopping Criteria** for training such as:\n",
    "   - when there is no improvement in the number of errors on the training data\n",
    "   - or after some fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "iris_x,iris_y = load_iris(return_X_y=True)\n",
    "preprocessor= MinMaxScaler()\n",
    "scaled_petal_data=preprocessor.fit_transform(X=iris_x[:,2:])\n",
    "#make binary labels\n",
    "is_setosa=np.where(iris_y==0,1,0)\n",
    "is_virginica = np.where(iris_y==2,1,0)\n",
    "is_versicolor= np.where(iris_y==1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make perceptrons for each class:not class problems\n",
    "setosa_classifier= Perceptron()\n",
    "print('Fitting Setosa Classifier')\n",
    "setosa_classifier.fit(scaled_petal_data,is_setosa,max_epochs=50,learning_rate=0.01)\n",
    "preds= setosa_classifier.predict(scaled_petal_data)\n",
    "print(f'accuracy= {(preds==is_setosa).sum()*100/150}%')\n",
    "\n",
    "virginica_classifier= Perceptron()\n",
    "print('\\nFitting Virginica Classifier')\n",
    "virginica_classifier.fit(scaled_petal_data,is_virginica,max_epochs=50,learning_rate=0.01)\n",
    "preds=virginica_classifier.predict(scaled_petal_data)\n",
    "print(f'accuracy= {(preds==is_virginica).sum()*100/150}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display the results\n",
    "fig,axs= plt.subplots(1,2,sharex=True,sharey=True)\n",
    "axs[0].set_title(\"setosa classifier\")\n",
    "axs[1].set_title(\"Virginica classifer\")\n",
    "utils.plot_decision_surface_v1(axs[0],setosa_classifier,scaled_petal_data,is_setosa)\n",
    "utils.plot_decision_surface_v1(axs[1],virginica_classifier,scaled_petal_data,is_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Going beyond single straight decision boundaries\n",
    "<div style=\"float: right; width: 50%\">\n",
    "<img align=\"right\", src=\"figures/mlp.png\", width = 75%/,height=40%/>\n",
    "    </div>Some problems need more than one single decision boundary, or  curved (non-linear) boundaries.\n",
    "\n",
    "So we need a more flexible architecture made out of the same building blocks.\n",
    "\n",
    "Multi-Layer Perceptrons (MLPs) have:\n",
    "* lots of connected perceptrons with trainable weights arranged in layers.\n",
    "* calculations flow layer-by-layer from inputs to outputs like a breath search\n",
    "\n",
    "At the output layers we know the targets and the computed activations \n",
    "* so we can use the perceptron training rule to adjust the last set of weights\n",
    "* But we need some adjustment to know how to adjust the weights from inputs to hidden layer nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change 1: use an  activation function that tells us what the actual input signal was\n",
    "<div style=\"float: right; width: 50%\"><img src=\"figures/sigmoid.png\", width = 80%/></div>\n",
    "Instead of using a step function which loses all the detail, we use something with one-to-one mapping\n",
    "\n",
    "Several options, most common is the *logistic* function $\\sigma(x)$\n",
    "\n",
    "$ output = \\sigma(input)$\n",
    "\n",
    "and using $x$ to stand for the input we have\n",
    "\n",
    "$ \\sigma(x)= \\frac{1}{ 1+e^{-x}}\\qquad\\qquad= \\frac{e^{x}}{ 1+e^{x}}$\n",
    "\n",
    " \n",
    " Which has the nice property that its *derivative* (gradient) is $\\sigma(x) (1- \\sigma(x))$\n",
    "\n",
    "Nowadays people also use **relu** activation: $relu(x) = MAX(0,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP Predicting output for a sample with logistic activation \n",
    "<img style=\"float:right\" width=50% src=\"figures/mlp-prediction.png\">\n",
    "\n",
    "**Input to node h1** = $w_1x_1 + w_3x_2$\n",
    "\n",
    "**Output from node h1** = $\\sigma(w_1x_1 +w_3x_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Input to node h2** = $w_2x_1 + w_4x_2$\n",
    "\n",
    "**Output from node h2** = $ \\sigma(w_2x_1 + w_4x_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Input to output node O** = $ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2)$\n",
    "\n",
    "**Output O** =   $ \\sigma( \\ w5\\ \\sigma(w_1x_1 + w_2x_2)  + w6\\ \\sigma(w_2x_1 + w_4x_2) ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change 2: _Back-propagation_ of errors according to their causes \n",
    "Since we don’t know what the ‘expected’ output of the hidden layer is, we cannot calculate the error.\n",
    "\n",
    "Instead, we share out the error from an output neuron to each hidden neurons.\n",
    "\n",
    "We do this in proportion to the strength of the signal coming from that hidden neuron. \n",
    "\n",
    "In practice we can feed in lots of samples then take the average of their errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLP: error back propagation for weight updates with a single sample     <img style=\"float:right\" width=40% src=\"figures/mlp-errors.png\">\n",
    "\n",
    "**One way of thinking about it**\n",
    "\n",
    "Each *epoch* \n",
    "- Loop over every sample\n",
    "  1. Update weights to final layer using 'real' difference between target and output\n",
    "     - change in w5 = error * output from H1 * learning rate   \n",
    "       = $E1 * \\sigma(w_1x_1 +w_2x_2) * \\alpha$  \n",
    "  \n",
    "     - change in w6 = error * output from H2  * learning rate.  \n",
    "        = $ E1 *  \\sigma(w_2x_1 + w_4x_2) * \\alpha$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "   2. Calculate share of error to feed back to hidden nodes\n",
    "      -  E2 = (signal to Output from h1)/ (total signal input to Output)  \n",
    "         = $\\frac {\\sigma(w_1x_1 +w_2x_2) }{ \\sigma(w_1x_1 +w_2x_2) +\\sigma(w_2x_1 + w_4x_2 )}$  \n",
    "      - similar for E3\n",
    "\n",
    "   3. Use these to update w1, w3 like perceptron training\n",
    "\n",
    "   4. Same process for rest of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### this is how it works conceptually - what about in practice?\n",
    "1. Doing updates after each item means the results would depend ( a lot) on which order you presented the items\n",
    "2. and it would be slow\n",
    "\n",
    "Get around first point by:\n",
    "- measuring and storing updates for each training item \n",
    "- then applying the average update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Get around the second point by doing this for *batches* of data rather than all at once.  \n",
    "- Each epoch we randomly split the training data into batches (typical size 32)\n",
    "- Then go through batch by batch, calculating all the updates **in parallel**, averaging then applying\n",
    "- This is called **Stochastic Gradient Descent**\n",
    " - updates happen $\\frac{training set size}{batch size}$ faster than updating only the end of an epoch when we've seen all the data\n",
    "- runs *batch_size* faster than doing one-by-one because of parallelism\n",
    "\n",
    "   \n",
    "   **if you can do it all in memory** (which is where GPUs and super-efficient libraries for vector/matrix algebra come in ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Change 3: We can have more than one output node\n",
    "- For a binary problem:\n",
    "    - we have *one* output node with logistic activation,\n",
    "     - and  interpret its output as the **probabilty** that the item belongs to class 1\n",
    "    - usually threshold at 0.5 to make a prediction, but probabilities can be informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- For a  problem with M classes:\n",
    "    - we usually have M output nodes\n",
    "    - 'onehot' encoding for y  \n",
    "       e.g. (M=4)  0 1 0 0  instead of '1'\n",
    "    - 'softmax' ('scaled all or nothing') activation function for final layer  \n",
    "      node outputs get scaled so they sum to 1,  \n",
    "      e.g. $(0.2 0.5 0.4 0.6) \\rightarrow (0.18, 0.29, 0.24, 0.35)$ \n",
    "      - use these values vs. the onehot encoding to get errors during training \n",
    "      - for making prediction choose index of output with  highest value : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- For regression problem:\n",
    "    - usually one output node with a linear activation  \n",
    "      e.g. output = summed weighted inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation: layers not nodes!\n",
    "In production code  we can represent the signals (inputs, each layer of hidden nodes, output nodes) as  vectors and the weights between each layer as a matrix. This means we can do the whole thing as a sequence of vector-matrix multiplications: **which is exactly what GPUs are designed to do extremely efficiently!!** \n",
    "\n",
    "So in the example above: \n",
    "$\\mathbf{x}=\\begin{pmatrix}x_1\\\\x_2\\\\\\end{pmatrix} \\qquad \\mathbf{h} = \\begin{pmatrix}h1\\\\h2\\\\\\end{pmatrix} \\qquad \\mathbf{o}=\\begin{pmatrix}o\\end{pmatrix}$\n",
    "\n",
    "the weights between layers are:\n",
    "$W_1 = \\begin{pmatrix}w_1, w_2\\\\w_3,w_4\\\\\\end{pmatrix} \\qquad W_2 = \\begin{pmatrix}w_5\\\\w_6\\\\\\end{pmatrix} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then by the definition of matrix-vector multiplication and how we apply a function to a vector, \n",
    "- the input to the layer of hidden nodes is $ \\mathbf{h} = W_1 \\cdot \\mathbf{x}$\n",
    "- so the output from the hidden layer is $logistic(\\mathbf{h})$\n",
    "- and we can write out the whole behaviour leading to the output $\\mathbf{o}$ as:\n",
    "\n",
    "$ \\qquad\\qquad \\mathbf{o} = logistic\\left( W_2\\cdot logistic(W_1 \\cdot \\mathbf{x}) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do I have to write this myself?\n",
    "\n",
    "As we mentioned above, it is 'cleanest' way to implement this using vector and matrix algebra,  \n",
    "and tools like Google's *tensorflow* will let you do exactly that\n",
    "\n",
    "Nowadays libraries like *PyTorch* and *keras* (part of tensorflow)  hide many of the details to provide a quick clean coding interface\n",
    "with *lots* of options\n",
    "- you'll see these in years 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For now we will use the sklearn implementation:  \n",
    "`class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', ...)`\n",
    "\n",
    "and show it working first for XOR then for the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "xor_X = np.asarray( [[0,0],[0,1],[1,0],[1,1]])\n",
    "xor_y = [0,1,1,0]#XOR\n",
    "\n",
    "\n",
    "# one hidden layer with 3 neurons logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "xorMLP = MLPClassifier( hidden_layer_sizes = (3),\n",
    "                        activation='logistic', solver='sgd',\n",
    "                        batch_size=1,learning_rate_init=0.1,\n",
    "                        n_iter_no_change=1000,random_state=1)\n",
    "xorMLP.fit(xor_X,xor_y)\n",
    "\n",
    "accuracy = 100*  xorMLP.score(xor_X,xor_y)\n",
    "print(f'Estimated accuracy is {accuracy} %')\n",
    "\n",
    "utils.plotDecisionSurface(xorMLP,xor_X,xor_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "onehot_labels = np.vstack((is_setosa, is_versicolor,is_virginica)).transpose()\n",
    "              \n",
    "# one hidden layer with 5 neurons logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "iris_MLP = MLPClassifier( hidden_layer_sizes = (10),activation='logistic',solver='lbfgs')#,random_state=5)\n",
    "iris_MLP.fit(scaled_petal_data,onehot_labels)\n",
    "\n",
    "accuracy = 100*  iris_MLP.score(scaled_petal_data,onehot_labels)\n",
    "print('Estimated accuracy using the training set is {} %'.format(accuracy))\n",
    "\n",
    "fig,axs = plt.subplots()\n",
    "utils.plot_decision_surface_v2(axs,iris_MLP,scaled_petal_data,onehot_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# And it's trivial to add more hidden nodes or add layers if we want to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Perceptrons  today\n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/2/26/Deep_Learning.jpg\" style=\"float:right\" width = 400>\n",
    "\n",
    "Since the 1980s Artificial Neural Networks have been hugely successful,   \n",
    "across a range of classification, regression and control problems.\n",
    "\n",
    "Only problems with MLPs were:\n",
    "- reliance on creating appropriate features \n",
    "- reliance on  appropriate scaling\n",
    "- lack of interpretablility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Neural Networks:   MLP with >5 hidden layers\n",
    "\n",
    "Typically complex early layers to find/create features such as:\n",
    "- Convolutional Layers discover spatial patterns e.g. 1D,   2D (images) N-D (video)  \n",
    "  each node acts like a small loical pattern detecting filter that 'sweeps over' the image \n",
    "- Recurrent Layers discover patterns in time e.g. speech recognition, natural language processing  \n",
    "  each node has a memory and (often) a 'forget-gate' that responds to patterns  \n",
    "  e.g. speech marks `\"`in text or `/*` and `*/` for comments in code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Because they have more weights to learn appropriate values for, Deep Networks need:\n",
    "- *lots* of data, \n",
    "- *lots* of computational power / time to train\n",
    "\n",
    "Deep Neural Networks useually have one or more  \"dense\" layers (like in a MLP) before the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple deep convolutional network for the MNIST data\n",
    "<img src=\"https://miro.medium.com/max/3744/1*SGPGG7oeSvVlV5sOSQ2iZw.png\" width = 1200>\n",
    "image from https://towardsdatascience.com/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "The perceptron training algorithm is guaranteed to converge on a solution – if one exists.  \n",
    " - But this will only be the case for linearly separable problems.\n",
    " - and if more than one decision voundary separates the data, there are no guarantees which you will get\n",
    "\n",
    "Multi-Layer Perceptrons solve this problem by combining perceptrons in a layered structure\n",
    "- The network is capable of learning making non-linear decision boundaries\n",
    "- Activation functions changed from a step, to something that preserves more information sigmoid (logistic)  \n",
    "  so you can make big[small] changes to weights leading to a hidden node if it's summed inputs were   lots [a little] over the 'trigger point'\n",
    "\n",
    "Backpropagation (Stochastic Gradient Descent)) is the algorithm used to train the more complex, multi layered networks\n",
    "- **signals propagate forwards** through the network.\n",
    "- **errors are propagated backwards** through the network. \n",
    "- For a given input, hidden nodes receive an error signal *in proportion to* the strength of their forwards signal\n",
    "- It is a form of local search so can get stuck on local optima \n",
    "\n",
    "Deep Neural Networks are MLP with extra layers to learn features from complex data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<h3>Intro to AI 2019-20 Jim Smith, UWE</h3>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
