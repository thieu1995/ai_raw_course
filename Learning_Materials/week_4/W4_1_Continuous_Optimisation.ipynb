{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dea5538-7e30-4824-a665-73c32d7f2e3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Week 4: Search in Continuous Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc327252-48f5-4fd2-bcfe-c345d34453c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Searching in different types of spaces\n",
    "\n",
    "- A candidate solution encodes values for a set of decisions that define a solution\n",
    "\n",
    "- examples so far: decisions take categorical values\n",
    "\n",
    "- so at every stage we have a fixed set of neighbours\n",
    "\n",
    "- What happens if the decisions take real/continuous values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d00ac0-0b1d-495f-aaa3-8fadb7cb5224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Continuous variables have infinite numbers of neighbours!\n",
    "\n",
    "- only limited by precision of floating points\n",
    "- so we can't examine all the neighbours.\n",
    "\n",
    "How do we make this work within our framework?\n",
    "\n",
    "- **Option1:** make neighbours by adding noise and just sample a few\n",
    " \n",
    "- **Option 2:** put the effort into using some cunning maths, and just generate one neighbour\n",
    "\n",
    "**Note**: This is  assuming we are doing *perturbative* search\n",
    "\n",
    "Examples: *lots* of machine learning algorithms, optimising design (dimensions) of physical things, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4853c-c062-475e-86aa-5c36810aff60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What does 'adding noise' mean?\n",
    "\n",
    "- generating and adding random numbers that follow some kind of pattern (distribution)\n",
    "- just as likely to be postivie as negative\n",
    "- small changes more likely than big ones\n",
    "\n",
    "Luckily there is a particular form of mathematical pattern that we observe when we collect data and plot *histograms* (frequency plots) in many different areas\n",
    " - physical science\n",
    "  - economics\n",
    "  - social science\n",
    "\n",
    "![normal distribution from samples](figs/normal_distribution_function.png)\n",
    " \n",
    "It's sometimes called a *Gaussian* distribution (after Carl Friedrich Gauss who first formalised it's properties in 1823)  \n",
    " but is so common that is mostly called the *Normal* distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a19fc-6000-45fb-b0f9-6fc6031bc7a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### You don't need to know this, but for the mathematically inclined, \n",
    "\n",
    "given  a distribution with mean $\\mu$ and *standard deviation* (width) $\\sigma$, \n",
    " - denoted $N(\\mu,\\sigma)$\n",
    " \n",
    "the probability of generating a value $\\leq x$  is described by the equation:  \n",
    "$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4b8d7-73fd-4396-a85b-3f4e7de495fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### But you should appreciate the properties of random numbers drawn from this distribution\n",
    "\n",
    " ![Probability of generating points from a random distribution](figs/Normal_Distribution_Sigma.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8ebd2-f755-4388-8a3c-3f56d051c9b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The use of noise  with these properties is so common that it is supported by commmon libraries in most languages \n",
    "\n",
    "e.g. in python ```` my_random_vars = random.normal(loc=0.0, scale=1.0, size=None)````\n",
    "\n",
    "- we can then change the values  to have different 'widths'  by **multiplying** by a constant $\\sigma$ \n",
    "- and to have a different average value  by **adding** a constant $\\mu$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c2ef9-f9fb-41b9-a2bc-4bad51bc1f70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Revisiting Option 1: Adding noise and sampling\n",
    "\n",
    "### Changing the Apply_move operator\n",
    "<div>\n",
    "    <p style=\"background:#F0FFFF;font-size:1em\">neighbour &larr; <b>ApplyMoveOperator</b> (working_candidate)</p>\n",
    "<ul>\n",
    "    <li>We add noise to each position independently</li>\n",
    "    <li>        We want the noise to Zero-mean   <br>\n",
    "        so a change up as likely as change down </li>\n",
    "    <li> We need to pick the Standard Deviation (sigma)  to suit scale of problem </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56feea2-6184-44f6-b9ac-4739dde8b8fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In code:\n",
    "````python\n",
    "neighbour = deepcopy(working_candidate)\n",
    "for decision in range ( len(neighbour.variable_values)):\n",
    "    randvar=np.random.normal(0,sigma,1)\n",
    "    neighbour.variable_values[decision] += randvar\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "### Might need some trial and error to decide how many samples to use in a neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b299dd2-99c1-4b0c-b9cd-2702ea655f00",
   "metadata": {},
   "source": [
    "### Simple $(1 +\\lambda)$ Evolution Strategy \n",
    "This is a slight modification of the basic  local search strategy\n",
    "- because we don't/ can't  search all of the neighbourhood,  \n",
    "   we keep going until we have some fixed number of iteratinos without improvement  \n",
    "   (instread of just one)\n",
    "\n",
    "- the green do is the current working candidate, \n",
    "- the blue points are the neighbours generated by adding random Gaussian noise\n",
    "- the red point is thre best neighbour which becoems the next working candidate\n",
    "![simple Evolution Strategy](figs/simplees.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349d416-94b0-43bf-a32d-d53e9dce4f52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Option 2: where possible\n",
    "\n",
    "1. Apply some maths to estimate local *gradient* (slope) of the cost function\n",
    "   - effectively asking the question  \n",
    "   'how much to change each decision to reduce the cost **from where we are now**'\n",
    "   - usually do this while calculating quality  \n",
    "      the gradient is a list or array with one value for each decision. \n",
    "      e.g.\n",
    "    ````python\n",
    "    cost, gradient = problem.evaluate(working_candidate)\n",
    "    ````\n",
    "   \n",
    "\n",
    "2. Then we have a Sample (neighbourhood) Size of 1 and\n",
    "<div>\n",
    "    <p style=\"background:#F0FFFF;font-size:1em\">neighbour &larr; <b>ApplyMoveOperator</b> (working_candidate)</p></div>\n",
    "    \n",
    "    Just means \n",
    "    \n",
    "    ````python\n",
    "    neighbour = deepcopy(working_candidate)\n",
    "    for decision in range ( len(neighbour.variable_values)):\n",
    "       neighbour.variable_values[decision] += gradient[decision]\n",
    "    ````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ea5d8-1fe9-40ce-a9d2-58df0617e31d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Steps of gradient descent on simple curve](figs/gradient_descent.png)\n",
    "[Image from]( https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0e315-a5ca-40cf-a724-6b66224ed486",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Three examples of paths followed by local search using gradient information\n",
    "![Example of paths taken by gradient descent on a landscape](figs/Gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33706ea-9795-41e8-9e67-8dc3dd1b58f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
