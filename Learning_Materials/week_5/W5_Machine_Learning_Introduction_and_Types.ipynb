{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Intelligence Topic 2 Machine Learning\n",
    "\n",
    "## Week 5: Introduction, Ethics and types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Week:\n",
    "###  Background\n",
    "- What is ML: recap\n",
    "- Types of ML\n",
    "- Ethical Considerations: Creating and Using Data\n",
    "\n",
    "###  Types of Machine Learning\n",
    "- Unsupervised Learning: K-Means as an example\n",
    "- Reinforcement Learning\n",
    "\n",
    "\n",
    "### Next few weeks: \n",
    "- Supervised Machine Learning, \n",
    "- Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:<img style=\"float:right\" width=300 src=\"./figures/basic-model-fitting.png\">\n",
    "- Machine Learning is:\n",
    " - the application of inductive logic to a dataset\n",
    " -  to create useful predictive models.  \n",
    " - So it is about solving  modelling problems\n",
    " \n",
    "\n",
    "- In week 1 we learned that problem solving is what you do when one of  \n",
    "  Input -> Model -> &  Output     is missing\n",
    "- In the third topic we will look at how we **manually** create models encoding human expertise  \n",
    "- Machine Learning is about how you **automatically** create models  from data (inputs and outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So it’s all about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Yes!\n",
    "\n",
    "The aim is to build ML systems that can be used to do things when data or scenarios arise.  \n",
    "So we need data to: \n",
    "- train them on,  \n",
    "- choose between models, \n",
    "- Know (estimate) how well they are going to do when we start using them\n",
    "\n",
    "We may not always have an output for every input \n",
    "- Because they’re not possible to capture\n",
    "  - e.g. data from scientific experiments such as genomics, astrophysics,...\n",
    "- Because sometimes we have to wait a while e.g.,\n",
    "  - game playing\n",
    "  - finding human volunteers to label images/ caption videos,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning\n",
    "Type | Inputs | Outputs | Feedback | What drives search? | Examples\n",
    "-----|--------|---------|----------------|---------------------|---------------------\n",
    "**Supervised** | Data |Predictions for each case | Correct labels | Accuracy of predictions made | **Recognition**  speech, images, actions, **Forecasting**\n",
    "**Reinforcement**| Scenarios | Actions to take in different states | Periodic Rewards | Expected future feedback | Learning game strategy\n",
    "**Unsupervised** | Data| Groupings of similar items | None | Statistics about cluster *coherence* and separation | Recommender systems, search engines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ethics: *Can* we use the data?\n",
    "- GDPR,  Privacy policies affect how it is collected\n",
    "- The law is very clear that we have to give people the right to:\n",
    "  - Provide Informed Consent about  how we are going to use their personal data at the time we collect it\n",
    "  - Find out what information we hold about them\n",
    "  - Withdraw their data (e.g. “right to forget”)\n",
    "\n",
    "- Examples of unethical use:    \n",
    " - Cambridge Analytica,   \n",
    " - targeting of fake news, propaganda on social media\n",
    "\n",
    "- Nowadays there should be clear collaboration agreements describing who is the data controller and who is the data processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethics: *Should* we use the data? <img src=\"figures/algorithms-of-oppression.png\" style=\"float:right\" width = 100> \n",
    "<img style=\"float:right\" src=\"figures/protected-characteristics.png\" width = 600>\n",
    "\n",
    "- ML is only as good as the data we give it\n",
    "- So we have to be very careful that the data is representative\n",
    "- Examples of problems:\n",
    "  - Microsoft's (abandoned) Tay Bot\n",
    "  - <a href=\"https://aibusiness.com/document.asp?doc_id=767688\">Google to remove gender labels from image recognition tool 25/2/21</a>\n",
    "  - <a href=\"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/\">Predictive policing algorithms are racist. They need to be dismantled.</a>\n",
    "  -  <a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G\">Amazon scraps secret AI recruiting tool that showed bias against women</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of bias in Image-Net\n",
    "Image net is the standard dataset used by many AI teams and researchers.  \n",
    "'Synsets' are sets of words with common meaning e.g. \"manager\", \"teacher\", \"cleaner\", \"nurse\", ...\n",
    "![Image net is the dataset used by many AI teams and researchers](https://www.image-net.org/static_files/figures/demographcs_distribution.png)\n",
    "\n",
    "Image from https://www.image-net.org/static_files/figures/demographcs_distribution.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quick Video from Cog-x\n",
    "\n",
    "[![AI Ethics with Dong Nguyen, The Alan Turing Institute | CogX17 Highlight | CogX](https://img.youtube.com/vi/v=M-ko82Y0GUQ/0.jpg)](https://www.youtube.com/watch?v=M-ko82Y0GUQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning : finding groups in data\n",
    "- We don’t have labels ... but we still want to find useful groups<img src=\"figures/clustering.png\" style=\"float:right\" width=400>\n",
    "- All data is defined in terms of values for features\n",
    "  - Numbers,  \n",
    "  - categories (_colour_, _name_, _Uni course_),  \n",
    "  - binary ( _present_,_absent_)\n",
    "- So we define distance measure _d(a,b)_ between two data items _a_ and _b_.\n",
    "  - Hamming Distance (number of features where _a_ and _b_ differ)\n",
    "  - Euclidean (straight line) distance for continuous numbers\n",
    "- Typically in clustering we look for a way of putting the data items into _k_ clusters  \n",
    "- As a search problem we are seeking models that maximise *Quality of Clustering*\n",
    "  - Minimise Intercluster Distance:  \n",
    "   max ( _d(a,b)_ ) for all _a,b_, in **same** cluster\n",
    "  - Maximise Intracluster distance:\n",
    "    min(_d(a,c)_) for _a_ and _c_ in **different** clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K Means: probably the best known clustering algorithm\n",
    "\n",
    "Basic Idea:  \n",
    "\n",
    "- clusters defined by a set of *centroids* (mid-points)\n",
    "  - centroids probably not \"real\" data items\n",
    "- data items assigned to the cluster with the closest centroid\n",
    "\n",
    "Algorithm\n",
    "- start with randomly picked items as  centroids\n",
    "- Loop until no changes:\n",
    "  - assign items to clusters\n",
    "  -  move each centroids to the new mid-point of all the items in the cluster\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means : Pseudocode- (not search based version)\n",
    "```\n",
    "#Step 0: init\n",
    "Pick K data points at random to be cluster “centroids” C_k k=1,...,K\n",
    "Set Converged = False\n",
    "  \n",
    "# Main loop\n",
    "WHILE Converged= False:  \n",
    "  ```\n",
    "\n",
    "```   \n",
    "    #Step 1: Assign items to clusters  \n",
    "    For each data point i:  \n",
    "      For each cluster k:\n",
    "        Calculate distance d(i,C_k) \n",
    "      Assign i to  cluster with smallest value of d(i,C_k) \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "    #Step 2: Check to see if the algorithm has converged\n",
    "    If no datapoints have moved cluster:\n",
    "       Converged = True\n",
    "    Else:\n",
    "       Remember new cluster membership\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "``` \n",
    "    #Step 3 Update cluster centroids\n",
    "    Foreach cluster k in (1...K):\n",
    "      Set new cluster centroid C_k =  mean position of  points in cluster\n",
    "      Update Cluster metrics\n",
    "      \n",
    "IF (converged==True):\n",
    "  return cluster centroids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### K-means as a search problem in *N* dimensions\n",
    "- CandidateSolution.variable_values:  \n",
    "   list of *N* x *K* values specifying the co-ordinates of *K* centroids\n",
    "- KMEANSproblem: \n",
    "    - reads in the data set when initialised\n",
    "    - quality  is calculated as sum of inter- and intra- cluster distances **for a given set of centroids**  \n",
    "      (i.e. a candidate solution)\n",
    "    - if we define the quality function carefully, we can calculate **gradient**  \n",
    "       - how to move each centroid to improve quality  \n",
    "- Solve KMeans by using gradient as the move operator in a local search algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by importing some modules we will use\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the apples-oranges-bananas data set from the first week\n",
    "\n",
    "# columns in X are Red,Green,Blue,Width,Height,Weight,Type\n",
    "\n",
    "# read in all the data from the apples-oranges-bananas dataset\n",
    "alldata = np.genfromtxt('data/fruit_values.csv', delimiter=',')\n",
    "\n",
    "#select the first two  feature values to make visualisation easier\n",
    "X = alldata[:,:2]\n",
    "\n",
    "numItems = X.shape[0]\n",
    "\n",
    "numFeatures  = X.shape[1]\n",
    "print(f\" The shape of X is {X.shape} so there are {numItems} items, each with {numFeatures} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AssignItemsToClusters(X, centres):\n",
    "    '''returns an array with index of the closest cluster for each item'''\n",
    "\n",
    "    # X is 2d array of data items, centres is 2d array of cluster centres\n",
    "    \n",
    "    # X.shape[0] and centres.shape[0] tell us the number of items and clusters\n",
    "    # X.shape[1] and centres.shape[1] tell us how many columns (features) they have\n",
    "    \n",
    "    assert X.shape[1] == centres.shape[1] #make sure they have same number of features\n",
    "    \n",
    "    #make 1D numpy array to hold an (integer) cluster label for each item\n",
    "    clusterLabels = np.zeros(X.shape[0],dtype=int)    \n",
    "    \n",
    "    # loop over all rows in the data array\n",
    "    for thisItem in range(X.shape[0]):\n",
    "        \n",
    "        # start by guessing  first cluster is closest\n",
    "        clusterLabels[thisItem] = 0\n",
    "        closestClusterDist = getDistance( X[thisItem], centres[0])\n",
    "        \n",
    "        #then loop over the other clusters looking for one closer\n",
    "        for thisCluster in range(1, centres.shape[0]):\n",
    "            clusterDist = getDistance( X[thisItem], centres[thisCluster])\n",
    "            if(clusterDist < closestClusterDist):\n",
    "                clusterLabels[thisItem] = thisCluster\n",
    "                closestClusterDist = clusterDist\n",
    "        \n",
    "    return clusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDistance(a,b):\n",
    "    ''' gets the Euclidean (straight line) distance between two items a and b'''\n",
    "    ''' this is just Pythagoras' theorem in N-dimensions'''\n",
    "    #a and b must have same number of dimensions/feastures\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    distance=0.0\n",
    "    for feature in range( a.shape[0]):\n",
    "        difference = a[feature] - b[feature]\n",
    "        distance= distance + difference*difference\n",
    "    return math.sqrt(distance)          \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PlotCluster(X,labels,centres, oldcentres,iteration):\n",
    "    fig, ax= plt.subplots(figsize=(7.5, 7.5))\n",
    "    \n",
    "    # show a scatter plot  coloured by labels\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "    #put the new centres on\n",
    "    for k in range(centres.shape[0]):\n",
    "        ax.plot(oldcentres[k][0],oldcentres[k][1], marker = '*',color='r',markersize=12)\n",
    "        \n",
    "    #draw lines to show movements\n",
    "    for k in range (centres.shape[0]):\n",
    "        ax.annotate(\"\", xy=centres[k], xycoords='data',xytext= oldcentres[k],textcoords='data',arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\",color='red'),)\n",
    "    ax.set_xlim((40,120))\n",
    "    ax.set_ylim((40,120))\n",
    "\n",
    "    plotTitle= \"Cluster membership and centroids: iteration \" + str(iteration) \n",
    "    ax.set_title(plotTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runKMEANS(X):\n",
    "    \n",
    "    #Step 0  pick K random data items as the initial cluster centres\n",
    "    #idx = np.linspace(start= 0,stop=X.shape[0],num=K,endpoint=False,dtype=int)\n",
    "    idx=index = np.random.choice(X.shape[0], K, replace=False)  \n",
    "    centres = copy.deepcopy(X[idx,:] ) #important that we make a separate copy rather than a reference\n",
    "    clusterLabels = AssignItemsToClusters(X,centres)\n",
    "\n",
    "    \n",
    "    for iteration in range(10):                    #MAIN LOOP\n",
    "        oldcentres= copy.deepcopy(centres) #remember for plotting\n",
    "        \n",
    "        # Step 1: assign items to  clusters\n",
    "        newLabels = AssignItemsToClusters(X,centres) \n",
    " \n",
    "        #step 2: see if algorithm has converged\n",
    "        numMoved = np.sum(newLabels != clusterLabels)\n",
    "        if( (iteration>0) and (numMoved==0)):\n",
    "            print(\"converged after {} iterations\".format(iteration))\n",
    "            break\n",
    "    \n",
    "        #step 3: find new cluster centroids\n",
    "        for k in range(K): # loop through each cluster\n",
    "            num_in_cluster = 0\n",
    "            for feature in range(X.shape[1]):\n",
    "                centres[k][feature]= 0.0\n",
    "            for item in range(numItems):\n",
    "                if (newLabels[item]==k) :\n",
    "                    num_in_cluster +=1\n",
    "                    centres[k] += X[item]\n",
    "            if(num_in_cluster >0):\n",
    "                centres[k] /= num_in_cluster\n",
    "        #plot     \n",
    "        PlotCluster(X,clusterLabels,centres,oldcentres, iteration+1) \n",
    "        # move cluster centroids\n",
    "        clusterLabels=newLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "runKMEANS(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Of course you wouldn't normally write your own version...\n",
    "\n",
    "We could easily do this within the framework from topic 1.\n",
    "\n",
    "BUT Highly optimised versions available in well-established frameworks e.g. Weka (Java), scikit-learn (python).\n",
    "\n",
    "`class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001,...)`  \n",
    " - defult number of clusters, variety of \"smart\" initialisation schemes\n",
    " - n_init: number of repeats it does before returning the best\n",
    " \n",
    "Object attributes include: \n",
    "- `cluster_centers` :ndarray of shape (n_clusters, n_features)\n",
    "- `labels`  : ndarray of shape (n_samples,)\n",
    "- `inertia` : float (Sum of squared distances of samples to their closest cluster center.)\n",
    " \n",
    "Methods include: \n",
    "- `fit(X[, y, sample_weight])` : Compute k-means clustering.\n",
    "or this estimator.\n",
    "- `predict(X[, sample_weight])` : Predict the closest cluster each sample in X belongs to.\n",
    "- `fit_predict(X[, y, sample_weight])` : Compute cluster centers and predict cluster index for each sample.\n",
    "- `get_params([deep])` : Get parameters of the model - including the cluster centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Strengths and weaknesses \n",
    "<img src=\"figures/kmeans_clustering_examples.png\" style=\"float:right\">\n",
    "\n",
    "### PROS: \n",
    "- fast, \n",
    "- lots of implementations\n",
    "\n",
    "### CONS:\n",
    "- need right value of K, \n",
    "- results depend on starting points\n",
    "\n",
    "### Assumptions:\n",
    "- all features are relevant, \n",
    "- data is \"globular\" with respect to the current features\n",
    "\n",
    "### How could we fix the counter-example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "Five minute video.\n",
    "\n",
    "[![Reinforcement learning for bar-tenders](https://img.youtube.com/vi/v=m2weFARriE8/0.jpg)](https://www.youtube.com/watch?v=m2weFARriE8)\n",
    "https://www.youtube.com/watch?feature=oembed&v=m2weFARriE8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning <img src=\"figures/RL.png\" style=\"float:right\" width=400>\n",
    "\n",
    "Q learning was best known initial algorithm\n",
    "\n",
    "Basic idea is that you have a *Reward* table R\n",
    "- which tells you what reward you get if you are in state s and take action a\n",
    "- for a multi-step problem the immediate rewards might be zero for many states  \n",
    "  e.g. finding your way out of a maze, playing tic-tac-toe (noughts and crosses)   \n",
    "\n",
    "Uses repeated trials to learn a Quality table Q: *s* rows and *a* actions  \n",
    "\n",
    "Start exploring, and build up a history of states (*s<sub>1</sub>*, *s<sub>2</sub>*, ..., *s<sub>t</sub>*) and actions (*a<sub>1</sub>*, *a<sub>2</sub>*, ..., *a<sub>t</sub>*)\n",
    "\n",
    "If at time  *t* you get a reward *r* then:  \n",
    "Q[s<sub>t</sub>][a<sub>t</sub>]  is increased by _r_    \n",
    "and the previous steps get a 'discounted' reward too: Q[s<sub>t-n</sub>][a<sub>t-n</sub>] is increased by  0.9<sup>*n*</sup> * _r_  \n",
    "\n",
    "\n",
    "Over time the Q table learns the best sequences of moves to take => use it to pick the next move\n",
    "    \n",
    "Problems with scalability as numbers of  states and actions increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Reinforcement Learning  <img src=\"figures/AlphaGoZero.png\" style=\"float:right\" width=300>\n",
    "- neural network rather than table\n",
    "- tends to learn “end-to-end”  rather than a Q table and a policy table\n",
    "  E.g. Alpha Go, Atari simulator\n",
    "- Relies on lots of data:  \n",
    "  e.g. Unity: ‘learning brain’ from ml-agents toolkit links out to tensorflow model\n",
    " \n",
    "- Alpha Go Zero:   \n",
    "  learned by playing itself!\n",
    "  image from https://medium.com/syncedreview/alphago-zero-approaching-perfection-d8170e2b4e48\n",
    " \n",
    " Really nice explanation of Q-learning here: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "Basic idea: Models divide up “decision space” into regions\n",
    "\n",
    "\n",
    "Search for model is driven by minimising error\n",
    "\n",
    "Form depends on what the ouputs can be\n",
    "- Two class:  0/1 loss (i.e. %age of wrong predictions)\n",
    "- Many class: Cross entropy \n",
    "- Continuous: mean squared error\n",
    "\n",
    "Types of models we’ll look at:\n",
    "- K Nearest Neighbours: make predictions based on *nearby* points\n",
    "- Greedy Rule Induction: use local search to learn a set of rules \n",
    "  - that don't make mistakes and correctly predict as many examples as possible\n",
    "- Decision Trees\n",
    "- Artificial Neural Nets\n",
    "\n",
    "**Classification** algorithms put labels on regions\n",
    "\n",
    "**Regression** algorithms compute a function in regions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The basic supervised learning process\n",
    "1. Choose features according to what kind of data you have available\n",
    "2. Decide what types of model might be appropriate (**CandidateSoliution** in search framework)\n",
    " - human readable?,   \n",
    " - type and amount of data?    \n",
    "3. Initialise Model \n",
    "4.  While not finished:\n",
    "  - See how well it does on training set (**Test** in our search framework)\n",
    " - Adapt model to try and reduce error on training set (**Generate** in our search framework)\n",
    "5.  Try to estimate how good it is (more accurate **Test**)\n",
    "\n",
    "Often do steps 3-5 above in parallel with different types of model or metaparameters\n",
    "E.g. max number of rules, max depth of trees, value of k in kNN, learning rates in ANN   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we use our data <img src=\"figures/using_data.png\" style=\"float:right\" width=300>\n",
    "\n",
    "### Unsupervised Learning: \n",
    "- Estimate of quality is based on whole dataset,  \n",
    "  so use it all for training\n",
    "\n",
    "### Reinforcement learning:\n",
    "- Problem is usually lack of data  \n",
    "  compared to size of state-action space.  \n",
    "- Because data is only generated by using the algorithm!\n",
    "- Alternate periods of:\n",
    "  - training (explore state-action-reward space to improve model)\n",
    "  - testing (choose current max predicted reward in each state) \n",
    "  \n",
    "### Supervised learning:\n",
    "Most commonly work in *off-line* or  *batch* mode \n",
    "- Random split of the data into separate test set, \n",
    "  training set, and sometimes validation set\n",
    "- Final model then built using all the data available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: you need to know and understand\n",
    "- When it is legal and ethical to use data\n",
    "- The basic workflow of **preprocess** &rarr; **train** &rarr; **test**\n",
    "- The difference between **Unsupervised**, **Reinforcement**, and **Supervised Learning**\n",
    "- kMeans as a typical unsupervised clustering algorithm\n",
    "  - stochastic, distance-based\n",
    "- The basic idea (but not the details) of reinforcement learning \n",
    "  - immediate rewards for taking action *a* in state *s*\n",
    "  - build up a Q-table predicting future reward for taking action *a* in state *s*\n",
    "- Supervised learning:\n",
    "  - Model represents a set of  *decision boundaries* that divide  feature space into regions\n",
    "  - *Classification*: each region has a label\n",
    "   - *Regression*: each region calculates a number\n",
    "  - adapts (optimises) boundaries to minimise error on the *training set*\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
